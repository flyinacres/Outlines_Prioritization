{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Guided Generation Test\nOriginal code is from here: https://github.com/dottxt-ai/outlines\n\nThere were some questionable things in this code, so I worked with Google Gemini\nto clean and explain the code a little better.\n\nAccording to Gemini the code was also out-of-date...","metadata":{}},{"cell_type":"code","source":"# Install the core libraries.\n# - outlines: The structured generation library\n# - transformers: The interface for downloading models\n# - accelerate: Helper for managing GPU device placement\n!pip install -q outlines transformers accelerate torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# TinyLlama is small (1.1B), standard, and widely supported.\n# It doesn't require \"trust_remote_code=True\".\nMODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\nprint(f\"Loading {MODEL_NAME}...\")\n\n# Load the model weights\nllm = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"cuda\",          # Load directly to GPU\n    torch_dtype=torch.float16,  # Use half-precision (standard for Llama models)\n    attn_implementation=\"eager\" # Force standard math (Prevents \"Flash Attention\" crashes on T4)\n)\n\n# Load the tokenizer (converts text to numbers)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Wrap the model with Outlines\n# This attaches the \"Finite State Machine\" engine that enforces your JSON schema.\nguided_model = outlines.from_transformers(llm, tokenizer)\n\nprint(\"Model loaded and wrapped successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T02:59:27.303374Z","iopub.execute_input":"2026-01-09T02:59:27.303698Z","iopub.status.idle":"2026-01-09T02:59:36.158085Z","shell.execute_reply.started":"2026-01-09T02:59:27.303671Z","shell.execute_reply":"2026-01-09T02:59:36.157521Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from enum import Enum\nfrom pydantic import BaseModel\nfrom typing import List\n\n# 1. Define the Schema\n# This acts as the \"stencil\" for the model. It cannot generate anything outside these rules.\nclass TicketPriority(str, Enum):\n    low = \"low\"\n    medium = \"medium\"\n    high = \"high\"\n    urgent = \"urgent\"\n\nclass ServiceTicket(BaseModel):\n    priority: TicketPriority\n    category: str\n    requires_manager: bool\n    summary: str\n    action_items: List[str]\n\n# 2. The Input Data\ncustomer_email = \"\"\"\nSubject: URGENT - Cannot access my account after payment\n\nI paid for the premium plan 3 hours ago and still can't access any features.\nI've tried logging out and back in multiple times. This is unacceptable as I\nhave a client presentation in an hour and need the analytics dashboard.\nPlease fix this immediately or refund my payment.\n\"\"\"\n\n# 3. The Prompt\n# TinyLlama expects this specific format. \n# We explicitly tell it to act as an API that outputs JSON.\nprompt = f\"\"\"<|system|>\nYou are a helpful assistant. Extract the support ticket details from the user email.\n</s>\n<|user|>\n{customer_email}\n</s>\n<|assistant|>\n\"\"\"\n\n# 4. Execution\nprint(\"Analyzing email...\")\n\n# The magic happens here:\n# The model tries to predict the next tokens, but 'outlines' filters out \n# any token that doesn't fit the ServiceTicket JSON structure.\nticket = guided_model(\n    prompt,\n    ServiceTicket,\n    max_new_tokens=512\n)\n\n# 5. Output\nprint(\"\\n--- TICKET CREATED ---\")\nprint(f\"Priority:      {ticket.priority.value.upper()}\")\nprint(f\"Category:      {ticket.category}\")\nprint(f\"Needs Manager: {ticket.requires_manager}\")\nprint(f\"Summary:       {ticket.summary}\")\nprint(\"Actions:\")\nfor item in ticket.action_items:\n    print(f\"- {item}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T03:00:03.699291Z","iopub.execute_input":"2026-01-09T03:00:03.700149Z","iopub.status.idle":"2026-01-09T03:00:03.800699Z","shell.execute_reply.started":"2026-01-09T03:00:03.700116Z","shell.execute_reply":"2026-01-09T03:00:03.800083Z"}},"outputs":[],"execution_count":5}]}