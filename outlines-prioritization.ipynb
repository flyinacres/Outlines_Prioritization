{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Guided Generation Test\nOriginal code is from here: https://github.com/dottxt-ai/outlines\n\nThere were some questionable things in this code, so I worked with Google Gemini\nto clean and explain the code a little better.\n\nAccording to Gemini the code was also out-of-date...","metadata":{}},{"cell_type":"code","source":"# Install the core libraries.\n# - outlines: The structured generation library\n# - transformers: The interface for downloading models\n# - accelerate: Helper for managing GPU device placement\n!pip install -q outlines transformers accelerate torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T03:55:55.360871Z","iopub.execute_input":"2026-01-10T03:55:55.361177Z","iopub.status.idle":"2026-01-10T03:56:00.487981Z","shell.execute_reply.started":"2026-01-10T03:55:55.361152Z","shell.execute_reply":"2026-01-10T03:56:00.487327Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# TinyLlama is small (1.1B), standard, and widely supported.\n# It doesn't require \"trust_remote_code=True\".\nMODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\nprint(f\"Loading {MODEL_NAME}...\")\n\n# Load the model weights\nllm = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"cuda\",          # Load directly to GPU\n    torch_dtype=torch.float16,  # Use half-precision (standard for Llama models)\n    attn_implementation=\"eager\" # Force standard math (Prevents \"Flash Attention\" crashes on T4)\n)\n\n# Load the tokenizer (converts text to numbers)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Wrap the model with Outlines\n# This attaches the \"Finite State Machine\" engine that enforces your JSON schema.\nguided_model = outlines.from_transformers(llm, tokenizer)\n\nprint(\"Model loaded and wrapped successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T03:56:04.545949Z","iopub.execute_input":"2026-01-10T03:56:04.546691Z","iopub.status.idle":"2026-01-10T03:56:44.706784Z","shell.execute_reply.started":"2026-01-10T03:56:04.546654Z","shell.execute_reply":"2026-01-10T03:56:44.705323Z"}},"outputs":[{"name":"stdout","text":"Loading TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebfe176723ef49dc90d1f20df258ab0b"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n2026-01-10 03:56:19.561021: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768017379.756227      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768017379.812906      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768017380.273065      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768017380.273114      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768017380.273117      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768017380.273119      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"998451ea7f334f89b78eef3097aaeef5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"867e5690ec0946a88f86a09f28a89894"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"999a6ba3df7443918bc2c685e17e5b2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d0c4a3634a6401a8e65ceb3594c8fbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d08d76e55bcd4506853871b4c232e052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5f0e452e1934b08984c3411bcc307b2"}},"metadata":{}},{"name":"stdout","text":"Model loaded and wrapped successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from enum import Enum\nfrom pydantic import BaseModel\nfrom typing import List\n\n# 1. Define the Schema\n# This acts as the \"stencil\" for the model. It cannot generate anything outside these rules.\nclass TicketPriority(str, Enum):\n    low = \"low\"\n    medium = \"medium\"\n    high = \"high\"\n    urgent = \"urgent\"\n\nclass ServiceTicket(BaseModel):\n    priority: TicketPriority\n    category: str\n    requires_manager: bool\n    summary: str\n    action_items: List[str]\n\n# 2. The Input Data\ncustomer_email = \"\"\"\nSubject: Critical - Cannot access my account after payment\n\nI paid for the premium plan 3 hours ago and still can't access any features.\nI've tried logging out and back in multiple times. This is unacceptable as I\nhave a client presentation in an hour and need the analytics dashboard.\nPlease fix this immediately or refund my payment.\n\"\"\"\n\n# 3. The Prompt\n# TinyLlama expects this specific format. \n# We explicitly tell it to act as an API that outputs JSON.\nprompt = f\"\"\"<|system|>\nYou are a helpful assistant. Extract the support ticket details from the user email.\n</s>\n<|user|>\n{customer_email}\n</s>\n<|assistant|>\n\"\"\"\n\n# 4. Execution\nprint(\"Analyzing email...\")\n\n# The magic happens here:\n# The model tries to predict the next tokens, but 'outlines' filters out \n# any token that doesn't fit the ServiceTicket JSON structure.\nticket = guided_model(\n    prompt,\n    ServiceTicket,\n    max_new_tokens=1024,\n    repetition_penalty=1.15 \n)\n\nticket_json = ServiceTicket.model_validate_json(ticket)\n\n# 5. Output\nprint(\"\\n--- TICKET CREATED ---\")\nprint(f\"Priority:      {ticket_json.priority.value.upper()}\")\nprint(f\"Category:      {ticket_json.category}\")\nprint(f\"Needs Manager: {ticket_json.requires_manager}\")\nprint(f\"Summary:       {ticket_json.summary}\")\nprint(\"Actions:\")\nfor item in ticket_json.action_items:\n    print(f\"- {item}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T04:09:30.335830Z","iopub.execute_input":"2026-01-10T04:09:30.336582Z","iopub.status.idle":"2026-01-10T04:09:53.004438Z","shell.execute_reply.started":"2026-01-10T04:09:30.336539Z","shell.execute_reply":"2026-01-10T04:09:53.003733Z"}},"outputs":[{"name":"stdout","text":"Analyzing email...\n\n--- TICKET CREATED ---\nPriority:      URGENT\nCategory:      support\nNeeds Manager: True\nSummary:       Repeatedly unable to access account after payment\nActions:\n- critical\n- cannotaccessaccountafterpayment\n- unavailablefeatures\n- unresolvedissue\n- urgentissues\n- urgentlyneedsattention\n- urgentissues\n- urgentissueswithoutsolution\n- urgentissueswithouttimeline\n- urgentissueswithin1day\n- urgentissueswithin5days\n- urgentissueswithin7days\n- urgentissueswithin1week\n- urgentissueswithin1month\n- urgentissueswithin1year\n- urgentissueswithin1hour\n- urgentissueswithin1minute\n- urgentissueswithin1second\n- urgentissueswithin1minuteand1second\n- urgentissueswithin1minuteand3seconds\n- urgentissueswithin1minuteand5seconds\n- urgentissueswithin1minuteand7seconds\n- urgentissueswithin1minuteand9seconds\n- urgentissueswithin1minuteand11seconds\n- urgentissueswithin1minuteand13seconds\n- urgentissueswithin1minuteand15seconds\n- urgentissueswithin1minuteand17seconds\n- urgentissueswithin1minuteand19seconds\n- urgentissueswithin1minuteand31seconds\n- urgentissueswithin1minuteand53seconds\n- urgentissueswithin1minuteand75seconds\n- urgentissueswithin1minuteand97seconds\n- urgentissueswithin1minuteand119seconds\n- urgentissueswithin1minuteand139seconds\n- urgentissueswithin1minuteand159seconds\n- urgentissueswithin1minuteand179seconds\n- urgentissueswithin1minuteand199seconds\n- urgentissueswithin1minuteand319seconds\n- urgentissueswithin1minuteand539seconds\n- urgentissueswithin1minuteand759seconds\n- urgentissueswithin1minuteand979seconds\n- urgentissueswithin1minuteand1199seconds\n- urgentissueswithin1minuteand1399seconds\n- urgentissueswithin1minuteand1599seconds\n- urgentissueswithin1minuteand1799seconds\n- urgentissueswithin1minuteand1999seconds\n- urgentissueswithin1minuteand3199seconds\n- urgentissueswithin1minuteand5399seconds\n- urgentissueswithin1minuteand7599seconds\n- urgentissueswithin1minuteand9799seconds\n- urgentissueswithin1minuteand11999seconds\n- urgentissueswithin1minuteand13999seconds\n- urgentissueswithin1minuteand15999seconds\n- urgentissueswithin1minuteand17999seconds\n- urgentissueswithin1minuteand19999seconds\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Gemini's explanation for the latest weird output:\n\nThis is a classic example of what we call a \"Semantic Repetition Loop\" facilitated by \"Syntactic Enabling.\"\nIt is hilarious to look at, but it reveals a fundamental limitation in how outlines interacts with smaller language models like TinyLlama.\nHere is the breakdown of why the \"Guardrails\" (Outlines) failed to stop the \"Drunk Driver\" (TinyLlama), and actually helped it stay in the loop.\n1. The Cause: \"The Loop\" (TinyLlama's Fault)\nTinyLlama (1.1B) is a very small model. When small models are uncertain, or when they generate a specific token pattern (like urgent-issues-within...), their internal attention mechanism gets \"stuck.\"\nThe model generated urgentissueswithin1minute.\nIt looked back at its own context, saw that pattern, and decided the most probable next token was and.\nThen it saw a number.\nIt fell into a rhythmic trap. To the model, this wasn't garbage; it was a high-probability mathematical sequence.\n2. The Enabler: \"The Valid Container\" (Outlines' Fault)\nThis is the specific interaction you asked about. You might wonder: Why didn't Outlines stop this?\nOutlines enforces Syntax, not Sanity.\nLook at your schema:\ncode\nPython\naction_items: List[str]\nOutlines converted this into a rule that looks roughly like this:\n\"The output must start with [. Then it must contain a String. Then a comma ,. Then another String. Repeat until ].\"\nAs the model started spewing \"urgentissueswithin1minute\", Outlines checked its rules:\nIs \"urgentissueswithin1minute\" a string? Yes.\nIs it valid JSON? Yes.\nDoes the schema limit the list to 5 items? No.\nSo, Outlines effectively said: \"This is perfectly valid data according to the rules you gave me. Keep going!\"\nThe library forced the model to stay on the road (valid JSON list), but it didn't stop the model from driving 500 miles past the destination.\n3. The Solution: Constrain the Container\nSince we can't make TinyLlama smarter (it is what it is), we must make the Schema stricter. We need to close the loophole that allows infinite lists.\nInstead of List[str], we should force the model to provide a fixed number of items. This breaks the loop because Outlines will force the model to write ] after the 3rd item, essentially \"cutting the mic.\"","metadata":{}}]}